{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6cfd538",
   "metadata": {},
   "source": [
    "# Multiple Imputation by Chained Equations (MICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241adef7",
   "metadata": {},
   "source": [
    "### DATA IMPUTATION METHOD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0367b6db",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Imputing a Single Dataset\n",
    "If you only want to create a single imputed dataset, you can use\n",
    "KernelDataSet:\n",
    "\n",
    "# Create kernel. \n",
    "kds = mf.KernelDataSet(\n",
    "  iris_amp,\n",
    "  save_all_iterations=True,\n",
    "  random_state=1991\n",
    ")\n",
    "\n",
    "# Run the MICE algorithm for 3 iterations\n",
    "kds.mice(3)\n",
    "\n",
    "# Return the completed kernel data\n",
    "completed_data = kds.complete_data()\n",
    "\n",
    "\n",
    "\n",
    "kernel_inplace.complete_data(dataset=0, inplace=True)\n",
    "print(iris_amp.isnull().sum(0))\n",
    "\n",
    "Diagnostic Plotting\n",
    "As of now, miceforest has four diagnostic plots available.\n",
    "\n",
    "Distribution of Imputed-Values\n",
    "We probably want to know how the imputed values are distributed. We can plot the original distribution beside the imputed distributions in each dataset by using the plot_imputed_distributions method of an ImputationKernel object:\n",
    "\n",
    "kernel.plot_imputed_distributions(wspace=0.3,hspace=0.3)\n",
    "\n",
    "The red line is the original data, and each black line are the imputed values of each dataset.\n",
    "\n",
    "Convergence of Correlation\n",
    "We are probably interested in knowing how our values between datasets converged over the iterations. The plot_correlations method shows you a boxplot of the correlations between imputed values in every combination of datasets, at each iteration. This allows you to see how correlated the imputations are between datasets, as well as the convergence over iterations:\n",
    "\n",
    "kernel.plot_correlations()\n",
    "\n",
    "Variable Importance\n",
    "We also may be interested in which variables were used to impute each variable. We can plot this information by using the plot_feature_importance method.\n",
    "\n",
    "kernel.plot_feature_importance(dataset=0, annot=True,cmap=\"YlGnBu\",vmin=0, vmax=1)\n",
    "\n",
    "The numbers shown are returned from the lightgbm.Booster.feature_importance() function. Each square represents the importance of the column variable in imputing the row variable.\n",
    "\n",
    "Mean Convergence\n",
    "If our data is not missing completely at random, we may see that it takes a few iterations for our models to get the distribution of imputations right. We can plot the average value of our imputations to see if this is occurring:\n",
    "\n",
    "kernel.plot_mean_convergence(wspace=0.3, hspace=0.4)\n",
    "\n",
    "Our data was missing completely at random, so we don’t see any convergence occurring here.\n",
    "\n",
    "Using the Imputed Data\n",
    "To return the imputed data simply use the complete_data method:\n",
    "\n",
    "dataset_1 = kernel.complete_data(0)\n",
    "\n",
    "The MICE Algorithm\n",
    "Multiple Imputation by Chained Equations ‘fills in’ (imputes) missing data in a dataset through an iterative series of predictive models. In each iteration, each specified variable in the dataset is imputed using the other variables in the dataset. These iterations should be run until it appears that convergence has been met.\n",
    "\n",
    "\n",
    "\n",
    "This process is continued until all specified variables have been imputed. Additional iterations can be run if it appears that the average imputed values have not converged, although no more than 5 iterations are usually necessary.\n",
    "\n",
    "\n",
    "\n",
    "Confidence Intervals:\n",
    "MICE can be used to impute missing values, however it is important to keep in mind that these imputed values are a prediction. Creating multiple datasets with different imputed values allows you to do two types of inference:\n",
    "\n",
    "Imputed Value Distribution: A profile can be built for each imputed value, allowing you to make statements about the likely distribution of that value.\n",
    "Model Prediction Distribution: With multiple datasets, you can build multiple models and create a distribution of predictions for each sample. Those samples with imputed values which were not able to be imputed with much confidence would have a larger variance in their predictions.\n",
    "\n",
    "\n",
    "\n",
    "Predictive Mean Matching\n",
    "miceforest can make use of a procedure called predictive mean matching (PMM) to select which values are imputed. PMM involves selecting a datapoint from the original, nonmissing data which has a predicted value close to the predicted value of the missing sample. The closest N (mean_match_candidates parameter) values are chosen as candidates, from which a value is chosen at random. This can be specified on a column-by-column basis. Going into more detail from our example above, we see how this works in practice:\n",
    "\n",
    "\n",
    "\n",
    "This method is very useful if you have a variable which needs imputing which has any of the following characteristics:\n",
    "\n",
    "Multimodal\n",
    "Integer\n",
    "Skewed\n",
    "Effects of Mean Matching\n",
    "\n",
    "We can see how our variables are distributed and correlated in the graph above. Now let’s run our imputation process twice, once using mean matching, and once using the model prediction.\n",
    "kernelmeanmatch = mf.ImputationKernel(ampdat, datasets=1,mean_match_candidates=5)\n",
    "kernelmodeloutput = mf.ImputationKernel(ampdat, datasets=1,mean_match_candidates=0)\n",
    "\n",
    "kernelmeanmatch.mice(2)\n",
    "kernelmodeloutput.mice(2)\n",
    "Let’s look at the effect on the different variables.\n",
    "\n",
    "With Mean Matching\n",
    "kernelmeanmatch.plot_imputed_distributions(wspace=0.2,hspace=0.4)\n",
    "\n",
    "You can see the effects that mean matching has, depending on the distribution of the data. Simply returning the value from the model prediction, while it may provide a better ‘fit’, will not provide imputations with a similair distribution to the original. This may be beneficial, depending on your goal."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
